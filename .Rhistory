install.packages("ggplot2")
data <- read.csv('basepro.csv', sep = ';', rowNames = TRUE)
# Big Data project for M2 EKAP
library(readxl)
library(gets)
library(glmnet)
library(corrplot)
library(tidyverse)
library(doParallel)
library(tseries)
library(lmtest)
library(ggplot2)
library(rpart)
library(randomForest)
library(gbm)
library(caret)
library(partykit)
library(PerformanceAnalytics)
library(FinTS)
library(diveRsity)
library(SIS)
library(msaenet)
library(ncvreg)
library(rbridge)
library(RRF)
setwd("/Users/Lucas/Desktop/Cours/Big Data")
data = data.frame(read_excel('data.xlsx')[1 : 980,])
summary(data)
y <- as.matrix(scale(data[, 2], center = TRUE, scale = FALSE))
y <- diff(y)
data <- data[- 1, ]
data$returns <- y
ggplot(mapping = aes(x = date, y = Index), data)+
geom_line(col = 'steelblue')+
theme_minimal()
ggplot(mapping = aes(x = date, y = returns), data)+
geom_line(col = 'steelblue')+
theme_minimal()
# Loop for stationarity tests
stat.tests <- function(data){
var_non_stat <<- data[- 1,]
for(i in 1 : ncol(data)){
adf <- FALSE
kpss <- FALSE
q <- FALSE
ar <- FALSE
if(adf.test(data[, i])$p.value > 0.05){
adf = TRUE
}
if(kpss.test(data[, i])$p.value < 0.05){
kpss = TRUE
}
if(Box.test(data[, i], lag = 1, type = c("Ljung-Box"))$p.value < 0.05){
q = TRUE
}
if(arima(x = y, order = c(1,0,0))[["coef"]][["ar1"]] > 0.7){
ar = TRUE
}
if(adf & kpss | adf & q | adf & ar | kpss & ar | kpss & q | ar & q == TRUE){
print(paste('variable :', names(data[i]), 'non-stationnaire'), sep = ' ')
var_non_stat[i] <<- diff(data[, i])
names(var_non_stat[i]) <<- names(data[, i])
}
}
}
# Checking for stationarity
stat.tests(data)
# Outliers detection
data_clean = var_non_stat
for (i in 1 : 28){
y=ts(var_non_stat[, i])
clean=Return.clean(y, method = "boudt")
clean=ts(clean)
data_clean[, i] <- clean
names(data_clean[i]) <- names(data[, i])
}
var_non_stat$num <- seq(1, 978, 1)
data_clean$num <- seq(1, 978, 1)
# Outliers plots
for (i in 1 : 28){
print(ggplot(mapping = aes(x = num, y = var_non_stat[, i]), var_non_stat)+
geom_line(color = 'red')+
geom_line(aes(x = num, y = data_clean[, i]), data = data_clean, col = 'steelblue')+
theme_minimal()+
ggtitle(names(data_clean[i]))+
ylab(names(data_clean[i]))+
xlab('time'))
}
# Desc. Statistics
data_clean <- data_clean[, -c(1, 2)]
for(i in 1:28){
print(names(data_clean[i]))
print(FinTS.stats(data_clean[, i]))
}
ybreaks <- seq(0,50,5)
ggplot(mapping = aes(x = returns), data_clean)+
geom_density(fill = 'steelblue')+
stat_function(fun = dnorm, args = list(mean = 2.5474, sd = 30.927), color = 'red')+
scale_x_continuous(limits = c(-100, 100))+
theme_minimal()
install.packages("regclass")
# Big Data project for M2 EKAP
library(readxl)
library(gets)
library(glmnet)
library(corrplot)
library(tidyverse)
library(doParallel)
library(tseries)
library(lmtest)
library(ggplot2)
library(rpart)
library(randomForest)
library(gbm)
library(caret)
library(partykit)
library(PerformanceAnalytics)
library(FinTS)
library(diveRsity)
library(SIS)
library(msaenet)
library(ncvreg)
library(rbridge)
library(RRF)
library(regclass)
setwd("C:/Users/Thiti/Desktop/M2 EKAP/S1/Econometrie big data/US stock returns-20201106")
data = data.frame(read_excel('data.xlsx')[1 : 980,])
summary(data)
#On transforme l'indice S&P en rentabilitÃ©
y <- as.matrix(data[, 2])
y <- diff(log(y))
data <- data[- 1, ]
data$returns <- y
ggplot(mapping = aes(x = date, y = Index), data)+
geom_line(col = 'steelblue')+
theme_minimal()
ggplot(mapping = aes(x = date, y = returns), data)+
geom_line(col = 'steelblue')+
theme_minimal()
# Loop for stationarity tests
stat.tests <- function(data){
var_non_stat <<- data[- 1,]
for(i in 1 : ncol(data)){
adf <- FALSE
kpss <- FALSE
q <- FALSE
ar <- FALSE
if(adf.test(data[, i])$p.value > 0.05){
adf = TRUE
}
if(kpss.test(data[, i])$p.value < 0.05){
kpss = TRUE
}
if(Box.test(data[, i], lag = 1, type = c("Ljung-Box"))$p.value < 0.05){
q = TRUE
}
if(arima(x = y, order = c(1,0,0))[["coef"]][["ar1"]] > 0.7){
ar = TRUE
}
if(adf & kpss | adf & q | adf & ar | kpss & ar | kpss & q | ar & q == TRUE){
print(paste('variable :', names(data[i]), 'non-stationnaire'), sep = ' ')
var_non_stat[i] <<- diff(data[, i])
names(var_non_stat[i]) <<- names(data[, i])
}
}
}
# Checking for stationarity
stat.tests(data)
# Outliers detection
data_clean = var_non_stat
for (i in 1 : 29){
y=ts(var_non_stat[, i])
clean=Return.clean(y, method = "boudt")
clean=ts(clean)
data_clean[, i] <- clean
names(data_clean[i]) <- names(data[, i])
}
var_non_stat$num <- seq(1, 978, 1)
data_clean$num <- seq(1, 978, 1)
# Outliers plots
for (i in 2 : 29){
print(ggplot(mapping = aes(x = num, y = var_non_stat[, i]), var_non_stat)+
geom_line(color = 'red')+
geom_line(aes(x = num, y = data_clean[, i]), data = data_clean, col = 'steelblue')+
theme_minimal()+
ggtitle(names(data_clean[i]))+
ylab(names(data_clean[i]))+
xlab('time'))
}
# Desc. Statistics
data_clean <- data_clean[, -c(1, 2)]
for(i in 1:28){
print(names(data_clean[i]))
print(FinTS.stats(data_clean[, i]))
}
ybreaks <- seq(0,50,5)
ggplot(mapping = aes(x = returns), data_clean)+
geom_density(fill = 'steelblue')+
stat_function(fun = dnorm, args = list(mean = 2.5474, sd = 30.927), color = 'red')+
scale_x_continuous(limits = c(-100, 100))+
theme_minimal()
ggplot(mapping = aes(x = returns), data_clean)+
geom_density(fill = 'steelblue')+
stat_function(fun = dnorm, args = list(mean = 0.005, sd = 0.0425), color = 'red')+
scale_x_continuous(limits = c(-100, 100))+
theme_minimal()
ggplot(mapping = aes(x = returns), data_clean)+
geom_density(fill = 'steelblue')+
stat_function(fun = dnorm, args = list(mean = 0.005, sd = 0.0425), color = 'red')+
scale_x_continuous(limits = c(-10, 10))+
theme_minimal()
ggplot(mapping = aes(x = returns), data_clean)+
geom_density(fill = 'steelblue')+
stat_function(fun = dnorm, args = list(mean = 0.005, sd = 0.0425), color = 'red')+
scale_x_continuous(limits = c(-1, 1))+
theme_minimal()
ggplot(mapping = aes(x = returns), data_clean)+
geom_density(fill = 'steelblue')+
stat_function(fun = dnorm, args = list(mean = 0.005, sd = 0.0425), color = 'red')+
scale_x_continuous(limits = c(-0.1, 0.1))+
theme_minimal()
ggplot(mapping = aes(x = returns), data_clean)+
geom_density(fill = 'steelblue')+
stat_function(fun = dnorm, args = list(mean = 0.005, sd = 0.0425), color = 'red')+
scale_x_continuous(limits = c(-0.3, 0.3))+
theme_minimal()
View(data_clean)
View(data)
View(data_clean)
View(data)
View(data_clean)
View(data)
View(data)
View(data)
View(data_clean)
print(data[969,1])
print(data_clean[975,1])
rownames(data_clean) <= NULL
View(data_clean)
rownames(data_clean) <= NULL
View(data_clean)
row.names(data_clean) <= NULL
View(data_clean)
row.names(data_clean) <- NULL
View(data_clean)
print(data[976,1])
print(data_clean)
print(data_clean[976,1])
data_clean <- data_clean[c(967 : 978), ]
x <- as.matrix(scale(data_clean[, - c(2, 18, 19, 27, 28)], center = TRUE, scale = TRUE)) #regressors CRSP and DP are equal to Y
y <- as.matrix(scale(data_clean[, 27], center = TRUE, scale = TRUE)) #Us stock returns
y <- as.numeric(y)
x_test <- x[783:978,]
y_test <- y[783:978]
x <- x[1:782,]
y <- y[1:782]
ridge <- cv.glmnet(x, y, alpha = 0, lambda = lambda, standardize = F, nfolds = 10)
plot(ridge)
best_lambda <- ridge$lambda.min
print(best_lambda)
# Fit w/ best lambda
opt_ridge <- glmnet(x, y, alpha = 0, lambda = best_lambda, standardize = F)
summary(opt_ridge)
# Exctracting coeffs
which(! coef(opt_ridge) == 0, arr.ind = TRUE)
round(opt_ridge$beta,4)
lasso <- cv.glmnet(x, y, alpha = 1, lambda = lambda, standardize = T, nfolds = 10)
plot(lasso)
best_lambda <- lasso$lambda.min
print(best_lambda)
# Fit w/ best lambda
opt_lasso <- glmnet(x, y, alpha = 1, lambda = best_lambda, standardize = T)
# Exctracting coeffs
which(! coef(opt_lasso) == 0, arr.ind = TRUE)
round(opt_lasso$beta,4)
x <- as.matrix(scale(data_clean[, - c(2, 18, 19, 27, 28)], center = TRUE, scale = TRUE)) #regressors CRSP and DP are equal to Y
y <- as.matrix(scale(data_clean[, 27], center = TRUE, scale = TRUE)) #Us stock returns
y <- as.numeric(y)
x_test <- x[783:978,]
y_test <- y[783:978]
x <- x[1:782,]
y <- y[1:782]
lasso <- cv.glmnet(x, y, alpha = 1, lambda = lambda, standardize = T, nfolds = 10)
plot(lasso)
best_lambda <- lasso$lambda.min
print(best_lambda)
# Fit w/ best lambda
opt_lasso <- glmnet(x, y, alpha = 1, lambda = best_lambda, standardize = T)
# Exctracting coeffs
which(! coef(opt_lasso) == 0, arr.ind = TRUE)
round(opt_lasso$beta,4)
