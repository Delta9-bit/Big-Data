install.packages("ggplot2")
# Big Data project for M2 EKAP
library(readxl)
library(gets)
library(glmnet)
library(corrplot)
library(tidyverse)
library(doParallel)
library(tseries)
library(lmtest)
library(ggplot2)
setwd("/Users/Lucas/Desktop/Cours/Big Data")
data = data.frame(read_excel('data.xlsx')[1 : 980,])
summary(data)
cor <- cor(data[, - c(1, 2)], use = 'complete.obs', method = c("spearman"))
corrplot(cor, method = "color")
cor_comp <- cor(data[, 2 : 28], use = 'complete.obs', method = c("spearman"))
corrplot(cor_comp, method = "color")
y <- as.matrix(scale(data[, 2], center = T, scale = F))
ggplot(mapping = aes(x = date, y = Index), data)+
geom_line(col = 'steelblue')+
theme_minimal()
x <- as.matrix(data[, - c(1,2)])
# Loop for stationarity tests
stat.tests <- function(data){
var_non_stat <<- data[- 1,]
for(i in 1 : ncol(data)){
adf <- FALSE
kpss <- FALSE
q <- FALSE
ar <- FALSE
if(adf.test(data[, i])$p.value > 0.05){
adf = TRUE
}
if(kpss.test(data[, i])$p.value < 0.05){
kpss = TRUE
}
if(Box.test(data[, i], lag = 1, type = c("Ljung-Box"))$p.value < 0.05){
q = TRUE
}
if(arima(x = y, order = c(1,0,0))[["coef"]][["ar1"]] > 0.7){
ar = TRUE
}
if(adf & kpss | adf & q | adf & ar | kpss & ar | kpss & q | ar & q == TRUE){
print(paste('variable :', names(data[i]), 'non-stationnaire'), sep = ' ')
var_non_stat[i] <<- diff(data[, i])
names(var_non_stat[i]) <<- names(data[, i])
}
}
}
mX <- data.matrix(data[, - c(1,2)])
# ARX model
ARX <- arx(y, mc = T, ar = 1, mxreg = mX, vcov.type = 'white')
View(data)
# ARX model
ARX <- arx(data$index, mc = T, ar = 1, mxreg = mX, vcov.type = 'white')
# ARX model
ARX <- arx(data$Index, mc = T, ar = 1, mxreg = mX, vcov.type = 'white')
View(mX)
View(data)
# ARX model
ARX <- arx(data$Index, mc = T, ar = 1, mxreg = mX[, 1:25], vcov.type = 'white')
mX <- data.matrix(data[, - c(1,2)])
# ARX model
ARX <- arx(data$Index, mc = T, ar = 1, mxreg = mX, vcov.type = 'white')
det(mX)
class(data[3:26])
class(mx)
CLASS(mX)
class(mX)
is.singular(mX)
# ARX model
ARX <- arx(data$Index, mc = T, ar = 1, mxreg = mX, vcov.type = 'white')
# ARX model
ARX <- arx(data$Index, mc = T, ar = 1, mxreg = x, vcov.type = 'white')
View(data)
y <- as.matrix(scale(data[, 2], center = T, scale = F))
y <- diff(y)
ggplot(mapping = aes(x = date, y = Index), data)+
geom_line(col = 'steelblue')+
theme_minimal()
ggplot(mapping = aes(x = date, y = diff(Index)), data)+
geom_line(col = 'steelblue')+
theme_minimal()
data$returns <- y
data <- data[- 1, ]
data$returns <- y
ggplot(mapping = aes(x = date, y = returns), data)+
geom_line(col = 'steelblue')+
theme_minimal()
# ARX model
ARX <- arx(data$Index, mc = T, ar = 1, mxreg = x, vcov.type = 'white')
# ARX model
ARX <- arx(data$returns, mc = T, ar = 1, mxreg = x, vcov.type = 'white')
# ARX model
ARX <- arx(data$returns, mc = T, ar = 1, vcov.type = 'white')
ARX
is.singular.matrix(x)
y <- as.matrix(scale(data[- 1, 2], center = T, scale = F))
y <- diff(y)
# ARX model
ARX <- arx(data$returns, mc = T, ar = 1,mxreg =  vcov.type = 'white')
# ARX model
ARX <- arx(data$returns, mc = T, ar = 1,mxreg = mX, vcov.type = 'white')
mX <- data.matrix(x)
View(mX)
# ARX model
ARX <- arx(data$returns, mc = T, ar = 1,mxreg = mX, vcov.type = 'white')
is.na(x)
which(is.na(x))
install.packages("rpart")
install.packages("randomForest")
install.packages("gbm")
install.packages("caret$")
install.packages("caret")
install.packages("partykit")
# Big Data project for M2 EKAP
library(readxl)
library(gets)
library(glmnet)
library(corrplot)
library(tidyverse)
library(doParallel)
library(tseries)
library(lmtest)
library(ggplot2)
library(rpart)
library(randomForest)
library(gbm)
library(caret)
library(partykit)
View(data)
rdf <- randomForest(Index, data, ntree = valntree, mtry = valmtry,
nodesize = valnodesize, important = TRUE, proximity = TRUE, nPerm = 1)
View(data)
rdf <- randomForest(returns ~ ., data, ntree = valntree, mtry = valmtry,
nodesize = valnodesize, important = TRUE, proximity = TRUE, nPerm = 1)
px <- ncol(data) - 1
valntree <- 2000
valmtry <- florr(sqrt(px))
valnodesize <- 1
valmtry <- floor(sqrt(px))
rdf <- randomForest(returns ~ ., data, ntree = valntree, mtry = valmtry,
nodesize = valnodesize, important = TRUE, proximity = TRUE, nPerm = 1)
print(rdf)
View(data)
plot(rdf)
tune.RF(x = data[, -c(1, 2, 29)], y = data[, 29], mtryStart = 2, ntreeTry = 500,
stepFactor = 1, improve = 0.001)
library(randomForest)
tune.RF(x = data[, -c(1, 2, 29)], y = data[, 29], mtryStart = 2, ntreeTry = 500,
stepFactor = 1, improve = 0.001)
tuneRF(x = data[, -c(1, 2, 29)], y = data[, 29], mtryStart = 2, ntreeTry = 500,
stepFactor = 1, improve = 0.001)
fit.control <- trainControl(method = 'repeatedcv', number = 5, repeats = 10,
classProbs = TRUE, summaryFunction = twoClassSummary,
search = 'grid')
rdf_grid <- train(Index ~ ., data = data, method = 'rf', metric = 'Accuracy',
tuneGrid = tune.mtry, trControl = fit.control)
rdf_grid <- train(Index ~ ., data = data, method = 'rf', metric = 'RMSE',
tuneGrid = tune.mtry, trControl = fit.control)
fit.control <- trainControl(method = 'repeatedcv', number = 5, repeats = 10, summaryFunction = twoClassSummary,
search = 'grid')
rdf_grid <- train(Index ~ ., data = data, method = 'rf', metric = 'RMSE',
tuneGrid = tune.mtry, trControl = fit.control)
tune.mtry <- expand.grid(.mtry = (1 : 10))
rdf_grid <- train(Index ~ ., data = data, method = 'rf', metric = 'RMSE',
tuneGrid = tune.mtry, trControl = fit.control)
fit.control <- trainControl(method = 'repeatedcv', number = 5, repeats = 10,
search = 'grid')
tune.mtry <- expand.grid(.mtry = (1 : 10))
rdf_grid <- train(Index ~ ., data = data, method = 'rf', metric = 'RMSE',
tuneGrid = tune.mtry, trControl = fit.control)
print(rdf_grid)
plot(rdf_grid)
tuneRF(x = data[, -c(1, 2, 29)], y = data[, 29], mtryStart = 2, ntreeTry = 500,
stepFactor = 1, improve = 0.001)
tuneRF(x = data[, -c(1, 2, 29)], y = data[, 29], mtryStart = 2, ntreeTry = 500,
stepFactor = 1, improve = 0.001)
tuneRF(x = data[, -c(1, 2, 29)], y = data[, 29], mtryStart = 1, ntreeTry = 500,
stepFactor = 1, improve = 0.001)
tuneRF(x = data[, -c(1, 2, 29)], y = data[, 29], mtryStart = 2, ntreeTry = 500,
stepFactor = 1, improve = 0.001)
tuneRF(x = data[, -c(1, 2, 29)], y = data[, 29], mtryStart = 1, ntreeTry = 500,
stepFactor = 1, improve = 0.001, trace = TRUE)
tuneRF(x = data[, -c(1, 2, 29)], y = data[, 29], mtryStart = 1, ntreeTry = 500,
stepFactor = 1, improve = 0.001, trace = TRUE, plot = TRUE)
fit.control <- trainControl(method = 'repeatedcv', number = 5, repeats = 10,
search = 'grid')
tune.mtry <- expand.grid(.mtry = (1 : 10))
rdf_grid <- train(Index ~ ., data = data, method = 'rf', metric = 'RMSE',
tuneGrid = tune.mtry, trControl = fit.control)
tuneRF(x = data[, -c(1, 2, 29)], y = data[, 29], mtryStart = 0, ntreeTry = 500,
stepFactor = 1, improve = 0.001, trace = TRUE, plot = TRUE)
tuneRF(x = data[, -c(1, 2, 29)], y = data[, 29], mtryStart = 1, ntreeTry = 500,
stepFactor = 1, improve = 0.0000000001, trace = TRUE, plot = TRUE)
tuneRF(x = data[, -c(1, 2, 29)], y = data[, 29], mtryStart = 2, ntreeTry = 500,
stepFactor = 1, improve = 0.0000000001, trace = TRUE, plot = TRUE)
tuneRF(x = data[, -c(1, 2, 29)], y = data[, 29], mtryStart = 3, ntreeTry = 500,
stepFactor = 1, improve = 0.0000000001, trace = TRUE, plot = TRUE)
tuneRF(x = x, y = data[, 29], mtryStart = 3, ntreeTry = 500,
stepFactor = 1, improve = 0.001, trace = TRUE, plot = TRUE)
tuneRF(x = data[, -c(29)], y = data[, 29], mtryStart = 3, ntreeTry = 500,
stepFactor = 1, improve = 0.001, trace = TRUE, plot = TRUE)
data[, -c(29)]
data[, 29]
View(data)
plot(data[, 29])
tuneRF(x = data[, 1 : 28], y = data[, 29], mtryStart = 3, ntreeTry = 500,
stepFactor = 1, improve = 0.001, trace = TRUE, plot = TRUE)
tuneRF(x = data[, 1 : 28], y = data[, 29], mtryStart = 1, ntreeTry = 500,
stepFactor = 1, improve = 0.001, trace = TRUE, plot = TRUE)
tuneRF(x = data[, 1 : 28], y = data[, 29], mtryStart = 1, ntreeTry = 500,
stepFactor = 1, improve = 0.001, trace = TRUE, plot = TRUE)
tuneRF(x = data[, 1 : 28], y = data[, 29], mtryStart = 2, ntreeTry = 500,
stepFactor = 1, improve = 0.001, trace = TRUE, plot = TRUE)
